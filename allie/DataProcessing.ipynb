{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Final Project - Data Processing"
      ],
      "metadata": {
        "id": "K_WhI_sYU_zg"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P__5t9aIAO4d"
      },
      "source": [
        "Run this cell to mount to google drive and ensure that cuda is available"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZB6WH74o_BDw"
      },
      "outputs": [],
      "source": [
        "import os, sys\n",
        "from google.colab import drive\n",
        "\n",
        "# help with mounting:\n",
        "# https://edstem.org/us/courses/69019/discussion/6372778?comment=14964937 (Ed #1086)\n",
        "# also other assignment notebooks\n",
        "drive.mount('/content/drive')\n",
        "%cd '/content/drive/MyDrive/OMSCS/finalproject/'\n",
        "\n",
        "sys.path.append('/content/drive/MyDrive/OMSCS/finalproject/')\n",
        "\n",
        "import torch\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(\"You are using device: %s\" % device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge-score"
      ],
      "metadata": {
        "id": "_ofW3USwfcjU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Cleaning\n",
        "\n",
        "Use the following code to import all necessary modules, set global data, and define a few utility methods.\n",
        "\n",
        "These utilities should be the only ones in a separate file."
      ],
      "metadata": {
        "id": "Gy-1Da8fR26L"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6DQhX2l9QvId"
      },
      "outputs": [],
      "source": [
        "# IMPORTS\n",
        "# only custom import is data_processor.py\n",
        "from data_processor import DataEncoder, Vocabulary, create_word_lookup_csv, create_clean_csv, get_encoded_bill_data\n",
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import yaml\n",
        "\n",
        "# these paths are for the original data\n",
        "data_csv = \"data/orig_data.csv\"\n",
        "train_csv = \"data/train_data.csv\"\n",
        "\n",
        "# these paths will be used to generate cleaner data\n",
        "clean_data_csv = \"data/orig_data_clean.csv\"\n",
        "clean_train_csv = \"data/train_data_clean.csv\"\n",
        "encoded_train_csv = \"data/encoded_train.csv\"\n",
        "\n",
        "# THESE CAN BE MODIFIED - add data path\n",
        "word_lookup_csv = \"word_lookup.csv\"\n",
        "\n",
        "# special tokens!\n",
        "special_tokens = ['<pad>', '<unk>', '<start>', '<stop>']\n",
        "pad_idx = 0\n",
        "train_percent = 0.8\n",
        "\n",
        "# cleaning methods\n",
        "def clean_all_data():\n",
        "    create_clean_csv(data_csv, clean_data_csv)\n",
        "    create_clean_csv(data_csv, clean_train_csv)\n",
        "\n",
        "\n",
        "def create_word_lookup(n_vocab=None, include_nums=True):\n",
        "    n = n_vocab\n",
        "    if n_vocab == None:\n",
        "      n = \"all\"\n",
        "    lookup_csv = f\"data/{n}_{word_lookup_csv}\"\n",
        "    create_word_lookup_csv(clean_data_csv, lookup_csv, special_tokens, n_vocab)\n",
        "    return lookup_csv\n",
        "\n",
        "\n",
        "def create_encoded_data(vocab):\n",
        "    d = DataEncoder(vocab, 2, 3)\n",
        "    d.encode_data_to_csv(clean_train_csv, encoded_train_csv)\n",
        "\n",
        "\n",
        "def get_config(config_file):\n",
        "    # get configurations\n",
        "    # source: assignment 4 code\n",
        "    with open(config_file, \"r\") as file:\n",
        "        config_dict = yaml.safe_load(file)\n",
        "    return config_dict\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Only run this cell if you need to rebuild any clean csv files."
      ],
      "metadata": {
        "id": "jwbDWGviTO-N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#clean_all_data()\n"
      ],
      "metadata": {
        "id": "zDgzngE0TORj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "H3UfLLQ2ZnKO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configuration\n",
        "\n",
        "Set all config variables. You could use a config file, but for debugging purposes, it's easier to set them here:"
      ],
      "metadata": {
        "id": "3OPFMdt6ZpTT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SET CONFIGS (easier than using config file)\n",
        "max_len = 256\n",
        "max_vocab = 1024"
      ],
      "metadata": {
        "id": "ZwoGF6wN6GYX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vocabulary and Data\n",
        "\n",
        "Run this code to get all necessary training data. Ensure that the config file is updated appropriately."
      ],
      "metadata": {
        "id": "bHR34NNWR-B5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a word lookup using ALL words - should only need to run once\n",
        "# always make all csv with ALL data, including numbers\n",
        "#all_lookup_csv = create_word_lookup()\n",
        "all_lookup_csv = \"data/all_word_lookup.csv\"\n",
        "all_vocab = Vocabulary(all_lookup_csv)\n",
        "print(all_lookup_csv)\n",
        "print(len(all_vocab))"
      ],
      "metadata": {
        "id": "UW6_O3IiARD4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create encoded data given all vocab - should only need to run once\n",
        "# always encode ALL data using ALL vocab\n",
        "#create_encoded_data(all_vocab)\n",
        "print(encoded_train_csv)"
      ],
      "metadata": {
        "id": "vx1I2N2eA0fZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a word lookup using ONLY top max_vocab words, may or may not include numbers\n",
        "#n_lookup_csv = create_word_lookup(max_vocab)\n",
        "n_lookup_csv = \"data/1024_word_lookup.csv\"\n",
        "n_vocab = Vocabulary(n_lookup_csv)\n",
        "len_vocab = len(n_vocab)\n",
        "print(n_lookup_csv)\n",
        "print(len_vocab)"
      ],
      "metadata": {
        "id": "TOqQPf5s4wNQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get data\n",
        "# data will be encoded with <unk> if it is out of the range of n_vocab\n",
        "train_data, valid_data = get_encoded_bill_data(encoded_train_csv, train_percent,\n",
        "                                               max_len, max_len, 0, 1, 3, len_vocab)\n",
        "print(\"train_data:\", len(train_data))\n",
        "print(\"valid_data:\", len(valid_data))\n",
        "\n"
      ],
      "metadata": {
        "id": "AVJEOXY9HZ1b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training / Translation Definitions\n",
        "\n",
        "Below are the definitions for training loops, evalations, and translations"
      ],
      "metadata": {
        "id": "j05wmSZsVT0J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#####################################################\n",
        "# Training Loops                                    #\n",
        "#####################################################\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from rouge_score import rouge_scorer, scoring\n",
        "\n",
        "# source for training:\n",
        "# https://pytorch.org/tutorials/beginner/introyt/trainingyt.html\n",
        "# Assignment 3: utils.py code\n",
        "def train_loop(model, train_dataloader, valid_dataloader, optimizer, loss_fn, epochs, device, vocab, plot_name):\n",
        "    plot_file = f\"plot_{plot_name}.png\"\n",
        "    avg_train_loss_scores = []\n",
        "    avg_valid_loss_scores = []\n",
        "\n",
        "    for e in range(epochs):\n",
        "        print(f\"----- EPOCH {e} -----\")\n",
        "\n",
        "        train_loss, avg_train_loss = train_single_epoch(model, train_dataloader, optimizer, loss_fn, device, vocab)\n",
        "        avg_train_loss_scores.append(avg_train_loss)\n",
        "        print(f\"Train Loss: {train_loss}, Average Loss: {avg_train_loss}\")\n",
        "\n",
        "        valid_loss, avg_valid_loss = evaluate_enc(model, valid_dataloader, loss_fn, device, vocab)\n",
        "        avg_valid_loss_scores.append(avg_valid_loss)\n",
        "        print(f\"Validation Loss: {valid_loss}, Average Loss: {avg_valid_loss}\")\n",
        "\n",
        "    print(\"Completed Train Loop\");\n",
        "    plot_and_print_data(epochs, avg_train_loss_scores, avg_valid_loss_scores, plot_file, \"loss\")\n",
        "\n",
        "\n",
        "def plot_and_print_data(epochs, train_data, valid_data, fname, plot_type):\n",
        "    # print data\n",
        "    print(f\"Train {plot_type}:\", train_data)\n",
        "    print(f\"Validation {plot_type}:\", valid_data)\n",
        "\n",
        "    # set labels\n",
        "    y_label = \"Error\"\n",
        "    if plot_type == \"loss\":\n",
        "      y_label = \"Average Loss\"\n",
        "    if plot_type == \"rouge\":\n",
        "      y_label = \"Rouge Scores\"\n",
        "\n",
        "    title_str = f\"{y_label} Over {epochs} Epochs\"\n",
        "    plot_file = f\"{plot_type}_{fname}\"\n",
        "\n",
        "    # plotting sources:\n",
        "    # https://pythonguides.com/python-plot-multiple-lines/\n",
        "    # https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.savefig.html\n",
        "    x = [i for i in range(epochs)]\n",
        "    plt.plot(x, train_data, label=\"Training\")\n",
        "    plt.plot(x, valid_data, label=\"Validation\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(y_label)\n",
        "    plt.legend()\n",
        "    plt.title(title_str)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(plot_file)\n",
        "    plt.clf()\n",
        "\n",
        "    print(f\"Plot Saved: {plot_file}\")\n",
        "\n",
        "\n",
        "# source for training:\n",
        "# https://pytorch.org/tutorials/beginner/introyt/trainingyt.html\n",
        "# Assignment 3: utils.py code\n",
        "def train_single_epoch(model, dataloader, optimizer, loss_fn, device, vocab):\n",
        "    total_loss = 0.\n",
        "    model.train()\n",
        "    for i, data in enumerate(dataloader):\n",
        "        # text shape is (batch_size, max_len_text)\n",
        "        # summary shape is (batch_size, max_len_summary)\n",
        "        text = data[0].to(device)\n",
        "        tgt_summary = data[1].long().to(device)\n",
        "        batch_size, max_len = tgt_summary.shape\n",
        "\n",
        "        if (i+1) % 100 == 0:\n",
        "          print(f\"batch {i+1}\")\n",
        "\n",
        "        # zero grad\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # get outputs\n",
        "        out_summary = model(text, tgt_summary)\n",
        "\n",
        "        if i == 0:\n",
        "          # sanity checks\n",
        "          print(f\"Train Tgt {i}:\",  translate_single_sample(vocab, tgt_summary[0]))\n",
        "          print(f\"Train Out {i}:\", translate_single_pred_sample(vocab, out_summary[0]))\n",
        "          print(\"out_summary shape\", out_summary.shape)\n",
        "          print(\"out_summary[0]\", out_summary[0])\n",
        "\n",
        "        # reshape\n",
        "        out_summary = out_summary.reshape(-1, out_summary.shape[-1])\n",
        "        tgt_summary = tgt_summary.reshape(-1)\n",
        "\n",
        "        # compute loss\n",
        "        loss = loss_fn(out_summary, tgt_summary)\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    return total_loss, avg_loss\n",
        "\n",
        "\n",
        "# source for evaluating:\n",
        "# Assignment 3: utils.py code\n",
        "def evaluate_enc(model, dataloader, loss_fn, device, vocab):\n",
        "    model.eval()\n",
        "    total_loss = 0.\n",
        "    with torch.no_grad():\n",
        "        for i, data in enumerate(dataloader):\n",
        "            # text shape is (batch_size, max_len_text)\n",
        "            # summary shape is (batch_size, max_len_summary)\n",
        "            text, summary = data\n",
        "            text = data[0].to(device)\n",
        "            tgt_summary = data[1].long().to(device)\n",
        "            batch_size, max_len = tgt_summary.shape\n",
        "\n",
        "            if (i+1) % 100 == 0:\n",
        "              print(f\"batch {i+1}\")\n",
        "\n",
        "            # get outputs\n",
        "            out_summary = model(text, tgt_summary)\n",
        "\n",
        "            if i == 0:\n",
        "              # sanity checks\n",
        "              print(f\"Valid Tgt {i}:\",  translate_single_sample(vocab, tgt_summary[0]))\n",
        "              print(f\"Valid Out {i}:\", translate_single_pred_sample(vocab, out_summary[0]))\n",
        "              print(\"out_summary shape\", out_summary.shape)\n",
        "              print(\"out_summary[0]\", out_summary[0])\n",
        "\n",
        "            # reshape\n",
        "            out_summary = out_summary.reshape(-1, out_summary.shape[-1])\n",
        "            tgt_summary = tgt_summary.reshape(-1)\n",
        "\n",
        "            # compute loss\n",
        "            loss = loss_fn(out_summary, tgt_summary)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    return total_loss, avg_loss\n"
      ],
      "metadata": {
        "id": "qJZnjPt34LIG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Definitions\n"
      ],
      "metadata": {
        "id": "tCwn6zrZVlVU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main Program"
      ],
      "metadata": {
        "id": "4EdU-6JFVpx5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sources\n"
      ],
      "metadata": {
        "id": "wFLDopdX1ano"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SOURCES\n",
        "# Assignment 3: utils.py code\n",
        "# Assignment 3: Machine_Translation.ipynb\n",
        "# Assignment 3 Transformer.py\n",
        "# partner Santiago's utils.py\n",
        "# https://edstem.org/us/courses/69019/discussion/6372778?comment=14964937\n",
        "# https://edstem.org/us/courses/69019\n",
        "# https://edstem.org/us/courses/69019/discussion/6188507\n",
        "# https://pytorch.org/tutorials/beginner/introyt/trainingyt.html\n",
        "# https://pythonguides.com/python-plot-multiple-lines/\n",
        "# https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.savefig.html\n",
        "# https://pytorch.org/tutorials/beginner/introyt/trainingyt.html\n",
        "# https://www.geeksforgeeks.org/python-string-join-method//discussion/6188507\n",
        "# https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html\n",
        "# https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "# https://github.com/pytorch/examples/blob/main/word_language_model/model.py\n",
        "# https://stackoverflow.com/questions/41488279/neural-network-always-predicts-the-same-class"
      ],
      "metadata": {
        "id": "PyM6vYr71aJQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}